{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud Detection with XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3MsF8uAXy66"
   },
   "source": [
    "**What is XGBoost?**\n",
    "\n",
    "* eXtreme Gradient Boosting (XGBoost) is a gradient-boosted decision tree (GBDT) machine learning library, and one of the most popular supervised machine learning algorithm. It is typically use to solve classification and regression problems.\n",
    "* XGBoost is an _ensemble learning algorithm_. The \"Gradient Boosting\" stems from combining multiple weak models to collectively form a stronger one. The process involves training a first model decision tree based on training data, then iteratively training an ensemble of shallow decision trees, using the residuals of each iteration to fit the next model. The final prediction is a weighted sum of all the tree predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-nRHgDiXy6-"
   },
   "source": [
    "## Dataset Overview\n",
    "\n",
    "This project utilises the IEEE-CIS-FRAUD-DATASET downloaded from Kaggle. Below are some descriptions of the dataset, posted by the original competition host: https://www.kaggle.com/competitions/ieee-fraud-detection/discussion/101203\n",
    "\n",
    "* TransactionDT: timedelta from a given refernce datetime\n",
    "* TransactionAMT: transactoin amount in USD\n",
    "* ProductCD: type of payment product used for transaction\n",
    "* card1 - card6: payment card information, such as card issuer, type, etc.\n",
    "* addr1 - addr2: address\n",
    "* dist1 - dist2: distance (eg. between billing address to mailing address, etc)\n",
    "* P_ and (R__) emaildomain: purchaser and recipient email domain\n",
    "* C1-C14: count\n",
    "* D1-D15: timedelta\n",
    "* M1-M19: match, such as names on card and address, etc.\n",
    "* Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-nRHgDiXy6-"
   },
   "source": [
    "### Step 0: Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zy8U1zbxXy6_"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r''\n",
    "df = pd.read_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of rows and columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Description\n",
    "Using df.head() we can observe the first few rows of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "i5nBye_gXy6_",
    "outputId": "1d884cc8-177b-4dfe-eec6-e8857dd049ad"
   },
   "outputs": [],
   "source": [
    "# Set maximum number of columns to display\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "\n",
    "# Explore the few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OjNX0ZghwVUR"
   },
   "outputs": [],
   "source": [
    "# Set the TransactionID to index as the TransactionID signifies a unique value corresponding each row of data\n",
    "df.set_index('TransactionID', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q6RwIuy7Xy6_"
   },
   "source": [
    "### Step 1: Understand the structure of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vymt41m1Xy6_"
   },
   "source": [
    "Before cleaning the data, it is important to first have an idea of what is included (or excluded, as missing values) in the dataset. As we have set TransactionID as the index, we should expect df.shape() to print 393 columns instead of 394 this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ICRr8jt4Xy7A",
    "outputId": "11e79e6e-0db3-4448-bb44-ff021717ce1b"
   },
   "outputs": [],
   "source": [
    "# Check the number of rows and columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q19tk2xxXy7A"
   },
   "source": [
    "Using df.dytpes.value_counts(), we assess the data type distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "vCPgyrZ0Xy7A",
    "outputId": "c94c48eb-ef55-4b8f-cace-2ea7f212f61c"
   },
   "outputs": [],
   "source": [
    "df.dtypes.value_counts() # count the number of types in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0cqJyJbXy7B"
   },
   "source": [
    "To get a quick overview of the data, including non-null counts, types, and memory usage, we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i0kdscvxXy7B",
    "outputId": "0715e7d4-92ae-4ab3-ee6a-347b7736bf1e"
   },
   "outputs": [],
   "source": [
    "# Summary of the dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fl0iPmVNXy7B"
   },
   "source": [
    "### Step 2: Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5TYdiiRcXy7B"
   },
   "source": [
    "Before training any model, cleaning and preprocessing data ensures consistency and efficiency in downstream tasks. Here, we will handle missing values, reduce memory usage, and convert data types.\n",
    "\n",
    "We will first identify columns with significant missingness using our custom check_missing function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zpsDVSbwXy7B",
    "outputId": "00c3df43-0d15-4edd-c458-c34f573389c5"
   },
   "outputs": [],
   "source": [
    "# Total number of rows\n",
    "total_rows = df.shape[0]\n",
    "total_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3mScJF5iXy7C"
   },
   "outputs": [],
   "source": [
    "def check_missing(df, dtype='object'): # This takes two parameters, df is the dataframe and the default value of 'object' \n",
    "    total_rows = df.shape[0]\n",
    "    # Check missing values and their percentage for specified datatype\n",
    "    missing_ = df.select_dtypes(include=dtype).isnull().sum()\n",
    "    missing_percentage = (missing_ / total_rows) * 100\n",
    "\n",
    "    # Combine into a df for better readability\n",
    "    missing_summary = pd.DataFrame({\n",
    "        'Missing Values' : missing_,\n",
    "        'Missing Percentage (%)' : missing_percentage})\n",
    "\n",
    "    # Filter out columns without missing values\n",
    "    missing_summary = missing_summary[missing_summary['Missing Values'] > 0]\n",
    "\n",
    "    # Sort by missing percentage\n",
    "    return missing_summary.sort_values(by='Missing Percentage (%)', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "urGKqvTHXy7C",
    "outputId": "7e0db69c-923b-43a0-9a0e-cff1e886f5c7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For numeric features\n",
    "check_missing(df, dtype='number') # We pass 'number' in dtype value, since both flaot and int realte to numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "jXEWwgEuXy7C",
    "outputId": "89f87aeb-86e6-4a84-9596-52f133b07574"
   },
   "outputs": [],
   "source": [
    "# For categorical features\n",
    "check_missing(df, dtype='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5x_qsJBXy7D"
   },
   "source": [
    "### 2.1 Handle Missing Values\n",
    "\n",
    "To address missing data robustly: for numerical features, we will use median imputation to fill $\\text{NaN}$ values, as the median is less sensitive to outliers than the mean. For categorical features, we will apply mode imputation to replace $\\text{NaN}$ values with the most frequently occurring category.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CpFZw2aXy7E"
   },
   "source": [
    "For categorical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6OewfGMSXy7E"
   },
   "outputs": [],
   "source": [
    "# Fill categorical columns with most frequent value (mode)\n",
    "cat_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "for col in cat_cols:\n",
    "  mode = df[col].mode(dropna=True)\n",
    "  if not mode.empty:\n",
    "    df.fillna({col:mode[0]}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pyPfBN1VtRya"
   },
   "source": [
    "**Confirm the Result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "t04H_wqxtUUO",
    "outputId": "b85e4594-c1af-46f2-c133-3ca0218e0248"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DHK4gFhxv2q"
   },
   "source": [
    "### 2.2 Handle Highly Correlated Features\n",
    "\n",
    "In statistics, correlation is a term that indicates the degree to which two variables move in relation to each other. Highly correlated features are variables that have a strong linear relationship to each other. In other words, if two features are highly correlated, it is likely that they carry similar information. \n",
    "\n",
    "Important to:\n",
    "\n",
    "    • Remove redundant features\n",
    "\n",
    "    • Reduce multicollinearity\n",
    "    \n",
    "    • Improve model efficiency\n",
    "\n",
    "**Compute correlation matrix**\n",
    "\n",
    "To calculate a correlation matrix, we can use the corr() function from the Pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T2YofSUS_nBg"
   },
   "outputs": [],
   "source": [
    "# Compute the correlation matrix (for numerical columns only)\n",
    "corr_matrix = df.select_dtypes(include=['number']).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "acd4AHxX_1q8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_high_correlations(corr_matrix, threshold=0.9): # Defining a function that takes two arguments, firstly, the corr_matrix that we calculated with the above code line, and a threshold of 0.9\n",
    "\t# Take the upper triangle of the correlation matrix without the diagonal\n",
    "\tupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)) # Filters\n",
    "\n",
    "\t# Find features with correlation greater than the threshold\n",
    "\thigh_corr = [(col, row, upper.loc[row, col])\n",
    "\tfor col in upper.columns\n",
    "\tfor row in upper.index\n",
    "\tif abs(upper.loc[row, col]) > threshold]\n",
    "\n",
    "\treturn sorted(high_corr, key=lambda x: -abs(x[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aMvyA5J8_8Ip",
    "outputId": "bfecb443-1f39-4670-d8e5-2953ab894adb"
   },
   "outputs": [],
   "source": [
    "# Then call it\n",
    "high_corr_pairs = get_high_correlations(corr_matrix, threshold=0.9)\n",
    "for feature1, feature2, corr_value in high_corr_pairs[:15]:\n",
    "  print(f\"{feature1} ↔ {feature2} = {corr_value:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03MgbnzIZzdh"
   },
   "source": [
    "**Drop Redundant Features (Optional)**\n",
    "\n",
    "If feature1 and feature2 are highly correlated, drop one of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2nsn0_U5Z5Qf"
   },
   "outputs": [],
   "source": [
    "# Uncomment, and run below code to drop one of the correlated features\n",
    "\"\"\"\n",
    "to_drop = set()\n",
    "for feature1, feature2, _ in high_corr_pairs:\n",
    "\tif feature1 not in to_drop:\n",
    "\t  to_drop.add(feature2)\n",
    "\n",
    "df.drop(columns=list(to_drop), inplace=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVPLWFSSv8je"
   },
   "source": [
    "### 2.3 Encoding Categorical Features\n",
    "\n",
    "Encoding categorical features means converting a category to a numerical value, reason being most machine learning models are designed to only take numerica data as input. \n",
    "\n",
    "- Checking cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rz6HCy61wKN3",
    "outputId": "cf893fb2-3318-4da4-e8b0-0d3de6b25e55"
   },
   "outputs": [],
   "source": [
    "# Select categorical columns\n",
    "cat_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Count unique values in each categorical column\n",
    "cardinality = df[cat_cols].nunique().sort_values(ascending=False)\n",
    "\n",
    "# Display the result\n",
    "print(cardinality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['P_emaildomain'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "brQPtIqba_FJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Label Encoding for high-cardinality feature\n",
    "le = LabelEncoder()\n",
    "df['P_emaildomain'] = le.fit_transform(df['P_emaildomain'])\n",
    "df['R_emaildomain'] = le.fit_transform(df['R_emaildomain'])\n",
    "\n",
    "df['P_emaildomain'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['P_emaildomain'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding for low-cardinality features\n",
    "df = pd.get_dummies(df, columns=['ProductCD','card4', 'card6', 'M4', 'M1', 'M2', 'M3', 'M5', 'M6', 'M7', 'M8', 'M9'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "id": "9dwLLey5cJh2",
    "outputId": "bd87b604-31cd-45c1-9890-aea1b951d737"
   },
   "outputs": [],
   "source": [
    "# Explore few rows again\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYyCz5ARwEPW"
   },
   "source": [
    "**Check imbalanced**\n",
    "\n",
    "Check the distribution of the target variable, `isFraud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "id": "aNFq_8v5wPsi",
    "outputId": "daaaf59f-60c0-4b6d-af88-8caa0ad11c12"
   },
   "outputs": [],
   "source": [
    "# Visualize class imbalance\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the class distribution\n",
    "sns.countplot(x='isFraud', data=df)\n",
    "plt.title('Distribution of Fraudulent vs Non-Fraudulent Transactions')\n",
    "plt.xlabel('Fraud or Not')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([0, 1], ['Not Fraud', 'Fraud'])\n",
    "plt.show()\n",
    "\n",
    "# Print percentage distribution\n",
    "fraud_rate = df['isFraud'].value_counts(normalize=True) * 100\n",
    "print(fraud_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zrT_Hh4xIdL"
   },
   "source": [
    "### Step 3: Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0xlAR_VXxrRr"
   },
   "source": [
    "**1: Prepare the Feature Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vaWtxQvDxVym"
   },
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop(columns=['isFraud'], axis=1)\n",
    "y = df['isFraud']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOEivnoDyCwv"
   },
   "source": [
    "**2: Train-Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6hpq0MdsyGNL"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRyi8OCcyvqB"
   },
   "source": [
    "**3: Model Pipeline with XGBoost**\n",
    "\n",
    "Train the XGBoost model using the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3pftlJqy6X-"
   },
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "! pip install xgboost\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MClJOSS20dhm"
   },
   "outputs": [],
   "source": [
    "# Define the Classifier\n",
    "classifier = XGBClassifier(eval_metric='logloss', random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGnDat9W1XYA"
   },
   "source": [
    "**Create the Training Pipeline**\n",
    "\n",
    "Now, we construct our machine learning pipeline using `ImbPipeline` from the `imblearn` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "idoIPzcH0m4a"
   },
   "outputs": [],
   "source": [
    "# Create training pipeline\n",
    "pipeline = ImbPipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', classifier)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCKIHHMn2AbI"
   },
   "source": [
    "**Train the Model**\n",
    "\n",
    "With our pipeline fully defined, we now train the model using the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "id": "Vt1MibkG2CX2",
    "outputId": "42561a8a-06f2-473c-82d5-b3a00f4d25e0"
   },
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train) # This initiates a complete training workflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ob8lFvTi3FL0"
   },
   "source": [
    "## Step 4: Predict and Evaluate\n",
    "\n",
    "Now that the model is trained, we predict on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Clb6Sg02CHt"
   },
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tadC3hEs3eth"
   },
   "source": [
    "**Evaluate the model performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "ATvffgbsqLCw",
    "outputId": "4f22048c-e842-4763-d833-e8b996b5362b"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Confusion matrix values from your result\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Labels for display\n",
    "labels = [\"Non-Fraud\", \"Fraud\"]\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=labels, yticklabels=labels)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Xca7mU22B7u",
    "outputId": "4fbecaf8-c7ae-4f73-a729-476b2aac81c6"
   },
   "outputs": [],
   "source": [
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
